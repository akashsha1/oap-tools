{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed69632",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.extraClassPath\":\"/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:///opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\", \n",
    "        \"spark.executor.extraClassPath\":\"/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:///opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.text.SimpleDateFormat;\n",
    "import java.util.Date\n",
    "import java.util.concurrent.Executors\n",
    "import java.util.concurrent.ExecutorService\n",
    "import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
    "import com.databricks.spark.sql.perf.Benchmark.ExperimentStatus\n",
    "\n",
    "val tpcds = new TPCDS (sqlContext = spark.sqlContext)\n",
    "val stream_num = 2\n",
    "val scaleFactor = \"1\"\n",
    "val iterations = 1 // how many times to run the whole set of queries.\n",
    "val format = \"parquet\"\n",
    "val useDecimal = true\n",
    "val query_filter = Seq() // Seq() == all queries\n",
    "//val query_filter = Seq(\"q1-v2.4\", \"q2-v2.4\") // run subset of queries\n",
    "val randomizeQueries = true // run queries in a random order. Recommended for parallel runs.\n",
    "val current_time = new SimpleDateFormat(\"yyyy-MM-dd-HH:mm:ss\").format(new Date)\n",
    "val resultLocation = s\"s3://aws-emr-resources-348941870272-us-east-2/results/tpcds_${format}/${scaleFactor}/${current_time}\"\n",
    "val databaseName = s\"tpcds_${format}_scale_${scaleFactor}_db\"\n",
    "val timeout = 60 // timeout in hours\n",
    "\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"10000\") // good idea for Q14, Q88.\n",
    "sql(s\"use $databaseName\")\n",
    "\n",
    "def queries = {\n",
    "    val filtered_queries = query_filter match {\n",
    "        case Seq() => tpcds.tpcds2_4Queries\n",
    "        case _ => tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name))\n",
    "    }\n",
    "    if (randomizeQueries) scala.util.Random.shuffle(filtered_queries) else filtered_queries\n",
    "}\n",
    "\n",
    "class ThreadStream(experiment:ExperimentStatus) extends Thread{\n",
    "    override def run(){\n",
    "        println(experiment.toString)\n",
    "        experiment.waitForFinish(timeout*60*60)\n",
    "    }\n",
    "}\n",
    "\n",
    "val threadPool:ExecutorService=Executors.newFixedThreadPool(stream_num)\n",
    "val experiments:Array[ExperimentStatus] = new Array[ExperimentStatus](stream_num)\n",
    "\n",
    "try {\n",
    "    for(i <- 0 to (stream_num - 1)){\n",
    "        experiments(i) = tpcds.runExperiment(\n",
    "            queries, \n",
    "            iterations = iterations,\n",
    "            resultLocation = resultLocation,\n",
    "            tags = Map(\"runtype\" -> \"benchmark\", \"database\" -> databaseName, \"scale_factor\" -> scaleFactor)\n",
    "        )\n",
    "        threadPool.execute(new ThreadStream(experiments(i)))\n",
    "    }\n",
    "}finally{\n",
    "    threadPool.shutdown()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84da8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
