{"cells":[{"cell_type":"markdown","source":["## Run BERT-Large inference workload\n\nIn the beginning, data preprocessing will take some minutes. Once the preprocessing is done, the inference workload can output throughput performance number in real time. It takes around 30 minutes to complete the entire inference process on Standard_F32s_v2 instance. The precise elapsed time depends on instance type and whether is Intel-optimized TensorFlow. \n\n**Note:** ***If you click \"Stop Execution\" for running training/inference cells, and then run training/inference again immediately. You may see lower performance number, because another training/inference is still on-going.***"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"915f3692-f0d0-44d9-a59e-59036fb24ee1"}}},{"cell_type":"code","source":["import os\nimport subprocess\n\nfrom pathlib import Path\n  \ndef run_inference():\n  inference = '/tmp/inference.sh'\n  with open(inference, 'w') as f:\n    f.write(\"\"\"#!/bin/bash\n    # BERT-Large Inference\n    # Install necessary package\n    sudo apt-get update\n    sudo apt-get install zip -y\n    sudo apt-get -y install git\n    sudo apt-get install -y numactl\n    # Remove old materials if exist\n    rm -rf /TF/\n    mkdir /TF/\n    # Create ckpt directory\n    mkdir -p /TF/BERT-Large-output/\n    export BERT_LARGE_OUTPUT=/TF/BERT-Large-output\n    # Download IntelAI benchmark\n    cd /TF/\n    wget https://github.com/IntelAI/models/archive/refs/tags/v1.8.1.zip\n    unzip v1.8.1.zip\n    cd /TF/models-1.8.1/\n    wget https://github.com/oap-project/oap-tools/raw/master/integrations/ml/databricks/benchmark/IntelAI_models_bertlarge_inference_realtime_throughput.patch\n    git apply IntelAI_models_bertlarge_inference_realtime_throughput.patch\n\n    export SQUAD_DIR=/dbfs/home/TF/bert-large/SQuAD-1.1/\n    export BERT_LARGE_DIR=/dbfs/home/TF/bert-large/\n    export PYTHONPATH=$PYTHONPATH:.\n\n    # Launch Benchmark for inference\n    numa_nodes=$(lscpu | awk '/^NUMA node\\(s\\)/{ print $3 }')\n\n    function run_inference_without_numabind() {\n      cd /TF/models-1.8.1/benchmarks/\n      python3 launch_benchmark.py \\\n        --model-name=bert_large \\\n        --precision=fp32 \\\n        --mode=inference \\\n        --framework=tensorflow \\\n        --batch-size=32 \\\n        --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n        --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n        --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n        --verbose \\\n        -- infer_option=SQuAD \\\n           DEBIAN_FRONTEND=noninteractive \\\n           predict_file=$SQUAD_DIR/dev-v1.1.json \\\n           experimental-gelu=False \\\n           init_checkpoint=model.ckpt-3649\n    }\n\n    function run_inference_with_numabind() {\n      cd /TF/models-1.8.1/benchmarks/\n      nohup python3 launch_benchmark.py \\\n        --model-name=bert_large \\\n        --precision=fp32 \\\n        --mode=inference \\\n        --framework=tensorflow \\\n        --batch-size=32 \\\n        --socket-id 0  \\\n        --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n        --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n        --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n        --verbose \\\n        -- infer_option=SQuAD \\\n           DEBIAN_FRONTEND=noninteractive \\\n           predict_file=$SQUAD_DIR/dev-v1.1.json \\\n           experimental-gelu=False \\\n           init_checkpoint=model.ckpt-3649 >> socket0-inference-log &\n\n       python3 launch_benchmark.py \\\n        --model-name=bert_large \\\n        --precision=fp32 \\\n        --mode=inference \\\n        --framework=tensorflow \\\n        --batch-size=32 \\\n        --socket-id 1 \\\n        --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n        --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n        --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n        --verbose \\\n        -- infer_option=SQuAD \\\n           DEBIAN_FRONTEND=noninteractive \\\n           predict_file=$SQUAD_DIR/dev-v1.1.json \\\n           experimental-gelu=False \\\n           init_checkpoint=model.ckpt-3649\n    }\n\n    if [ \"$numa_nodes\" = \"1\" ];then\n            run_inference_without_numabind\n    else\n            run_inference_with_numabind\n    fi\"\"\")\n    \n  os.chmod(inference, 555)\n  p = subprocess.Popen([inference], stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  directory_to_second_numa_info = Path(\"/sys/devices/system/node/node1\")\n\n  \n  if  directory_to_second_numa_info.exists():\n    # 2 NUMA nodes\n    for line in iter(p.stdout.readline, ''):\n      if b'Reading package lists...' in line or b'INFO:tensorflow:tokens' in line or b'INFO:tensorflow:  name = bert' in line:\n        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n      if b\"INFO:tensorflow:examples/sec\" in line:\n        print(\"\\t\\t\\t\\t  Inference started, current real-time throughput (examples/sec) : \" + str(float(str(line).strip(\"\\\\n'\").split(' ')[1])*2), end='\\r')\n      if b\"throughput((num_processed_examples-threshod_examples)/Elapsedtime)\" in line:\n        print(\"\\t\\t\\t\\t  Inference finished, overall inference throughput (examples/sec) : \" + str(float(str(line).strip(\"\\\\n'\").split(':')[1])*2), end='\\r')\n      if line == b'' and p.poll() != None:\n        break\n  else:\n    # 1 NUMA node\n    for line in iter(p.stdout.readline, ''):\n      if b'Reading package lists...' in line or b'INFO:tensorflow:tokens' in line or b'INFO:tensorflow:  name = bert' in line:\n        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n      if b\"INFO:tensorflow:examples/sec\" in line:\n        print(\"\\t\\t\\t\\t  Inference started, current real-time throughput (examples/sec) : \" + str(line).strip(\"\\\\n'\").split(' ')[1], end='\\r')\n      if b\"throughput((num_processed_examples-threshod_examples)/Elapsedtime)\" in line:\n        print(\"\\t\\t\\t\\t  Inference finished, overall inference throughput (examples/sec) : \" + str(line).strip(\"\\\\n'\").split(':')[1], end='\\r')\n      if line == b'' and p.poll() != None:\n        break\n       \n  p.stdout.close()\n  \nrun_inference()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a631848-2959-4fed-8510-465edf8f6103"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Check whether is Intel-optimized TensorFlow\n\nThis is a simple auxiliary script tool to check whether the installed TensorFlow is Intel-optimized TensorFlow. \"Ture\" represents Intel-optimized TensorFlow."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8470d925-add1-4b7f-a520-3ce253e93a1c"}}},{"cell_type":"code","source":["# Print version, and check whether is intel-optimized\nimport tensorflow\nprint(\"tensorflow version: \" + tensorflow.__version__)\n\nfrom packaging import version\nif (version.parse(\"2.5.0\") <= version.parse(tensorflow.__version__)):\n  from tensorflow.python.util import _pywrap_util_port\n  print( _pywrap_util_port.IsMklEnabled())\nelse:\n  from tensorflow.python import _pywrap_util_port\n  print(_pywrap_util_port.IsMklEnabled())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ba0549c-cd3b-4b43-9735-5b2b32431035"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"benchmark_tensorflow_bertlarge_inference","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":323111285860384}},"nbformat":4,"nbformat_minor":0}
