{"cells":[{"cell_type":"code","source":["import os\nimport subprocess\n\nfrom pathlib import Path\n  \ndef run_training():\n  training = '/tmp/training.sh'\n  with open(training, 'w') as f:\n    f.write(\"\"\"#!/bin/bash\n    # BERT-Large Training\n    # Install necessary package\n    sudo apt-get update\n    sudo apt-get install zip -y\n    sudo apt-get -y install git\n    sudo apt-get install -y libblacs-mpi-dev\n    sudo apt-get install -y numactl\n    \n    # Remove old materials if exist\n    rm -rf /TF/\n    mkdir /TF/\n    # Create ckpt directory\n    mkdir -p /TF/BERT-Large-output/\n    # Download IntelAI benchmark\n    cd /TF/\n    wget https://github.com/IntelAI/models/archive/refs/tags/v1.8.1.zip\n    unzip v1.8.1.zip\n    \n    cores_per_socket=$(lscpu | awk '/^Core\\(s\\) per socket/{ print $4 }')\n    numa_nodes=$(lscpu | awk '/^NUMA node\\(s\\)/{ print $3 }')\n    export SQUAD_DIR=/dbfs/home/TF/bert-large/SQuAD-1.1\n    export BERT_LARGE_MODEL=/dbfs/home/TF/bert-large/wwm_uncased_L-24_H-1024_A-16\n    export BERT_LARGE_OUTPUT=/TF/BERT-Large-output/\n    export PYTHONPATH=$PYTHONPATH:.\n    \n    function run_training_without_numabind() {\n     python launch_benchmark.py \\\n        --model-name=bert_large \\\n        --precision=fp32 \\\n        --mode=training \\\n        --framework=tensorflow \\\n        --batch-size=4 \\\n        --benchmark-only \\\n        --data-location=$BERT_LARGE_MODEL \\\n        -- train-option=SQuAD  DEBIAN_FRONTEND=noninteractive   config_file=$BERT_LARGE_MODEL/bert_config.json   init_checkpoint=$BERT_LARGE_MODEL/bert_model.ckpt     vocab_file=$BERT_LARGE_MODEL/vocab.txt train_file=$SQUAD_DIR/train-v1.1.json     predict_file=$SQUAD_DIR/dev-v1.1.json      do-train=True learning-rate=1.5e-5   max-seq-length=384     do_predict=True warmup-steps=0     num_train_epochs=0.1     doc_stride=128      do_lower_case=False     experimental-gelu=False     mpi_workers_sync_gradients=True\n    }\n\n    function run_training_with_numabind() {\n      intra_thread=`expr $cores_per_socket - 2`\n      python launch_benchmark.py \\\n        --model-name=bert_large \\\n        --precision=fp32 \\\n        --mode=training \\\n        --framework=tensorflow \\\n        --batch-size=4 \\\n        --mpi_num_processes=$numa_nodes \\\n        --num-intra-threads=$intra_thread \\\n        --num-inter-threads=1 \\\n        --benchmark-only \\\n        --data-location=$BERT_LARGE_MODEL \\\n        -- train-option=SQuAD  DEBIAN_FRONTEND=noninteractive   config_file=$BERT_LARGE_MODEL/bert_config.json init_checkpoint=$BERT_LARGE_MODEL/bert_model.ckpt     vocab_file=$BERT_LARGE_MODEL/vocab.txt train_file=$SQUAD_DIR/train-v1.1.json     predict_file=$SQUAD_DIR/dev-v1.1.json      do-train=True learning-rate=1.5e-5   max-seq-length=384     do_predict=True warmup-steps=0     num_train_epochs=0.1     doc_stride=128      do_lower_case=False     experimental-gelu=False     mpi_workers_sync_gradients=True\n    }\n    \n    # Launch Benchmark for training\n    cd /TF/models-1.8.1/benchmarks/\n    \n    if [ \"$numa_nodes\" = \"1\" ];then\n            run_training_without_numabind\n    else\n            run_training_with_numabind\n    fi \"\"\")\n    \n  os.chmod(training, 555)\n  p = subprocess.Popen([training], stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  directory_to_second_numa_info = Path(\"/sys/devices/system/node/node1\")\n  \n  if  directory_to_second_numa_info.exists():\n    # 2 NUMA nodes\n    for line in iter(p.stdout.readline, ''):\n      if b\"Reading package lists...\" in line or b\"answer: [UNK] 1848\" in line:\n        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n      if b\"INFO:tensorflow:examples/sec\" in line:\n        print(\"\\t\\t\\t\\t  Training started, current real-time throughput (examples/sec) : \" + str(float(str(line).strip(\"\\\\n'\").split(' ')[1])*2), end='\\r')\n      if line == b'' and p.poll() != None:\n        break\n  else:\n    # 1 NUMA node\n    for line in iter(p.stdout.readline, ''):\n      if b\"Reading package lists...\" in line or b\"answer: [UNK] 1848\" in line:\n        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n      if b\"INFO:tensorflow:examples/sec\" in line:\n        print(\"\\t\\t\\t\\t  Training started, current real-time throughput (examples/sec) : \" + str(line).strip(\"\\\\n'\").split(' ')[1], end='\\r')\n      if line == b'' and p.poll() != None:\n        break\n        \n  p.stdout.close()\n\nrun_training()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96f083b7-e3a3-4cdf-8474-7f304b641fd7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Print version, and check whether is intel-optimized\nimport tensorflow\nprint(\"tensorflow version: \" + tensorflow.__version__)\n\nfrom packaging import version\nif (version.parse(\"2.5.0\") <= version.parse(tensorflow.__version__)):\n  from tensorflow.python.util import _pywrap_util_port\n  print( _pywrap_util_port.IsMklEnabled())\nelse:\n  from tensorflow.python import _pywrap_util_port\n  print(_pywrap_util_port.IsMklEnabled())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3ba1baf-4329-4b05-8071-12ccec717f16"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"benchmark_tensorflow_bertlarge_training","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2725057430102142}},"nbformat":4,"nbformat_minor":0}
