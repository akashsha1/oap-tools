{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d16614b5-65b7-418c-8c1d-34b872f89027","showTitle":false,"title":""}},"source":["## BERT-Large benchmark for Tensorflow\n","\n","We use [Model Zoo](https://github.com/IntelAI/models) to run [BERT Large](https://github.com/IntelAI/models/tree/v1.8.1/benchmarks/language_modeling/tensorflow/bert_large/README.md) model on SQuADv1.1 datasets."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cd9e2afe-f00b-44b2-bf9d-28624fafd372","showTitle":false,"title":""}},"source":["## Part 1. Download datasets, checkpoints and pre-trained model\n","\n","Download datasets, checkpoints and pre-trained model from the Internet to Databricks File System (DBFS). These data can share across clusters and only need to download once. So if you run multiple copyed notebooks simultaneously, please ensure run this cell only once to avoid unpredicted issues."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"24028691-c8a6-40a1-945d-6449823d20d1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sh\n","\n","# Download datasets, checkpoints and pre-trained model\n","rm -rf /bench/TF/bert-large\n","mkdir -p /bench/TF/bert-large\n","\n","mkdir -p /bench/TF/bert-large/SQuAD-1.1\n","cd /bench/TF/bert-large/SQuAD-1.1\n","wget https://github.com/oap-project/oap-project.github.io/raw/master/resources/ai/bert/dev-v1.1.json\n","wget https://github.com/oap-project/oap-project.github.io/raw/master/resources/ai/bert/evaluate-v1.1.py\n","wget https://github.com/oap-project/oap-project.github.io/raw/master/resources/ai/bert/train-v1.1.json\n","\n","cd /bench/TF/bert-large\n","wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/bert_large_checkpoints.zip\n","unzip bert_large_checkpoints.zip\n","\n","cd /bench/TF/bert-large\n","wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n","unzip wwm_uncased_L-24_H-1024_A-16.zip"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"54a4594b-ee41-4530-bd6c-a6ddaa661ce6","showTitle":false,"title":""}},"source":["### After the data have downloaded, you can start the BERT-Large training/inference workload."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"96ce1b45-0f14-4eb2-b5f1-539af2495c73","showTitle":false,"title":""}},"source":["## Part 2. Run BERT-Large training workload\n","\n","In the beginning, data preprocessing will take some minutes. Once the preprocessing is done, the training workload can output throughput performance number in real time. It takes around five hours to complete the entire training process on Standard_F32s_v2 instance. The precise elapsed time depends on instance type and whether is Intel-optimized TensorFlow. \n","\n","\n","**Note:** ***If you click \"Stop Execution\" for running training/inference cells, and then run training/inference again immediately. You may see lower performance number, because another training/inference is still on-going.***"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"48189df1-04ac-4b37-a46c-8affe60d34e6","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["import os\n","import subprocess\n","\n","from pathlib import Path\n","  \n","def run_training():\n","  training = '/tmp/training.sh'\n","  with open(training, 'w') as f:\n","    f.write(\"\"\"#!/bin/bash\n","    # BERT-Large Training\n","    # Install necessary package\n","    sudo apt-get update\n","    sudo apt-get install zip -y\n","    sudo apt-get -y install git\n","    sudo apt-get install -y libblacs-mpi-dev\n","    sudo apt-get install -y numactl\n","    \n","    # Remove old materials if exist\n","    rm -rf /TF/\n","    mkdir /TF/\n","    # Create ckpt directory\n","    mkdir -p /TF/BERT-Large-output/\n","    # Download IntelAI benchmark\n","    cd /TF/\n","    wget https://github.com/IntelAI/models/archive/refs/tags/v1.8.1.zip\n","    unzip v1.8.1.zip\n","    \n","    cores_per_socket=$(lscpu | awk '/^Core\\(s\\) per socket/{ print $4 }')\n","    numa_nodes=$(lscpu | awk '/^NUMA node\\(s\\)/{ print $3 }')\n","    export SQUAD_DIR=/bench/TF/bert-large/SQuAD-1.1\n","    export BERT_LARGE_MODEL=/bench/TF/bert-large/wwm_uncased_L-24_H-1024_A-16\n","    export BERT_LARGE_OUTPUT=/TF/BERT-Large-output/\n","    export PYTHONPATH=$PYTHONPATH:.\n","    \n","    function run_training_without_numabind() {\n","     python launch_benchmark.py \\\n","        --model-name=bert_large \\\n","        --precision=fp32 \\\n","        --mode=training \\\n","        --framework=tensorflow \\\n","        --batch-size=4 \\\n","        --benchmark-only \\\n","        --data-location=$BERT_LARGE_MODEL \\\n","        -- train-option=SQuAD  DEBIAN_FRONTEND=noninteractive   config_file=$BERT_LARGE_MODEL/bert_config.json   init_checkpoint=$BERT_LARGE_MODEL/bert_model.ckpt     vocab_file=$BERT_LARGE_MODEL/vocab.txt train_file=$SQUAD_DIR/train-v1.1.json     predict_file=$SQUAD_DIR/dev-v1.1.json      do-train=True learning-rate=1.5e-5   max-seq-length=384     do_predict=True warmup-steps=0     num_train_epochs=0.1     doc_stride=128      do_lower_case=False     experimental-gelu=False     mpi_workers_sync_gradients=True\n","    }\n","\n","    function run_training_with_numabind() {\n","      intra_thread=`expr $cores_per_socket - 2`\n","      python launch_benchmark.py \\\n","        --model-name=bert_large \\\n","        --precision=fp32 \\\n","        --mode=training \\\n","        --framework=tensorflow \\\n","        --batch-size=4 \\\n","        --mpi_num_processes=$numa_nodes \\\n","        --num-intra-threads=$intra_thread \\\n","        --num-inter-threads=1 \\\n","        --benchmark-only \\\n","        --data-location=$BERT_LARGE_MODEL \\\n","        -- train-option=SQuAD  DEBIAN_FRONTEND=noninteractive   config_file=$BERT_LARGE_MODEL/bert_config.json init_checkpoint=$BERT_LARGE_MODEL/bert_model.ckpt     vocab_file=$BERT_LARGE_MODEL/vocab.txt train_file=$SQUAD_DIR/train-v1.1.json     predict_file=$SQUAD_DIR/dev-v1.1.json      do-train=True learning-rate=1.5e-5   max-seq-length=384     do_predict=True warmup-steps=0     num_train_epochs=0.1     doc_stride=128      do_lower_case=False     experimental-gelu=False     mpi_workers_sync_gradients=True\n","    }\n","    \n","    # Launch Benchmark for training\n","    cd /TF/models-1.8.1/benchmarks/\n","    \n","    if [ \"$numa_nodes\" = \"1\" ];then\n","            run_training_without_numabind\n","    else\n","            run_training_with_numabind\n","    fi \"\"\")\n","    \n","  os.chmod(training, 555)\n","  os.system(\"ps -Af | grep launch_benchmark.py | grep -v grep | awk '{print $2}' | xargs kill -9\")\n","  os.system(\"ps -Af | grep run_squad.py | grep -v grep | awk '{print $2}' | xargs kill -9\")\n","  p = subprocess.Popen([training], stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","  directory_to_second_numa_info = Path(\"/sys/devices/system/node/node1\")\n","  \n","  if  directory_to_second_numa_info.exists():\n","    # 2 NUMA nodes\n","    for line in iter(p.stdout.readline, ''):\n","      if b\"Reading package lists...\" in line or b\"answer: [UNK] 1848\" in line:\n","        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n","      if b\"INFO:tensorflow:examples/sec\" in line:\n","        print(\"\\t\\t\\t\\t  Training started, current real-time throughput (examples/sec) : \" + str(float(str(line).strip(\"\\\\n'\").split(' ')[1])*2), end='\\r')\n","      if line == b'' and p.poll() != None:\n","        break\n","  else:\n","    # 1 NUMA node\n","    for line in iter(p.stdout.readline, ''):\n","      if b\"Reading package lists...\" in line or b\"answer: [UNK] 1848\" in line:\n","        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n","      if b\"INFO:tensorflow:examples/sec\" in line:\n","        print(\"\\t\\t\\t\\t  Training started, current real-time throughput (examples/sec) : \" + str(line).strip(\"\\\\n'\").split(' ')[1], end='\\r')\n","      if line == b'' and p.poll() != None:\n","        break\n","        \n","  p.stdout.close()\n","\n","run_training()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5e68a552-7863-4ce9-aa60-eab40415a006","showTitle":false,"title":""}},"source":["## Part 3. Run BERT-Large inference workload\n","\n","In the beginning, data preprocessing will take some minutes. Once the preprocessing is done, the inference workload can output throughput performance number in real time. It takes around 30 minutes to complete the entire inference process on Standard_F32s_v2 instance. The precise elapsed time depends on instance type and whether is Intel-optimized TensorFlow. \n","\n","**Note:** ***If you click \"Stop Execution\" for running training/inference cells, and then run training/inference again immediately. You may see lower performance number, because another training/inference is still on-going.***"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c88e041c-0634-40d0-9f76-b1d39f910c89","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["import os\n","import subprocess\n","\n","from pathlib import Path\n","  \n","def run_inference():\n","  inference = '/tmp/inference.sh'\n","  with open(inference, 'w') as f:\n","    f.write(\"\"\"#!/bin/bash\n","    # BERT-Large Inference\n","    # Install necessary package\n","    sudo apt-get update\n","    sudo apt-get install zip -y\n","    sudo apt-get -y install git\n","    sudo apt-get install -y numactl\n","    # Remove old materials if exist\n","    rm -rf /TF/\n","    mkdir /TF/\n","    # Create ckpt directory\n","    mkdir -p /TF/BERT-Large-output/\n","    export BERT_LARGE_OUTPUT=/TF/BERT-Large-output\n","    # Download IntelAI benchmark\n","    cd /TF/\n","    wget https://github.com/IntelAI/models/archive/refs/tags/v1.8.1.zip\n","    unzip v1.8.1.zip\n","    cd /TF/models-1.8.1/\n","    wget https://github.com/oap-project/oap-tools/raw/master/integrations/ml/databricks/benchmark/IntelAI_models_bertlarge_inference_realtime_throughput.patch\n","    git apply IntelAI_models_bertlarge_inference_realtime_throughput.patch\n","\n","    export SQUAD_DIR=/bench/TF/bert-large/SQuAD-1.1/\n","    export BERT_LARGE_DIR=/bench/TF/bert-large/\n","    export PYTHONPATH=$PYTHONPATH:.\n","\n","    # Launch Benchmark for inference\n","    numa_nodes=$(lscpu | awk '/^NUMA node\\(s\\)/{ print $3 }')\n","\n","    function run_inference_without_numabind() {\n","      cd /TF/models-1.8.1/benchmarks/\n","      python3 launch_benchmark.py \\\n","        --model-name=bert_large \\\n","        --precision=fp32 \\\n","        --mode=inference \\\n","        --framework=tensorflow \\\n","        --batch-size=32 \\\n","        --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n","        --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n","        --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n","        --verbose \\\n","        -- infer_option=SQuAD \\\n","           DEBIAN_FRONTEND=noninteractive \\\n","           predict_file=$SQUAD_DIR/dev-v1.1.json \\\n","           experimental-gelu=False \\\n","           init_checkpoint=model.ckpt-3649\n","    }\n","\n","    function run_inference_with_numabind() {\n","      cd /TF/models-1.8.1/benchmarks/\n","      nohup python3 launch_benchmark.py \\\n","        --model-name=bert_large \\\n","        --precision=fp32 \\\n","        --mode=inference \\\n","        --framework=tensorflow \\\n","        --batch-size=32 \\\n","        --socket-id 0  \\\n","        --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n","        --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n","        --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n","        --verbose \\\n","        -- infer_option=SQuAD \\\n","           DEBIAN_FRONTEND=noninteractive \\\n","           predict_file=$SQUAD_DIR/dev-v1.1.json \\\n","           experimental-gelu=False \\\n","           init_checkpoint=model.ckpt-3649 >> socket0-inference-log &\n","\n","       python3 launch_benchmark.py \\\n","        --model-name=bert_large \\\n","        --precision=fp32 \\\n","        --mode=inference \\\n","        --framework=tensorflow \\\n","        --batch-size=32 \\\n","        --socket-id 1 \\\n","        --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n","        --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n","        --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n","        --verbose \\\n","        -- infer_option=SQuAD \\\n","           DEBIAN_FRONTEND=noninteractive \\\n","           predict_file=$SQUAD_DIR/dev-v1.1.json \\\n","           experimental-gelu=False \\\n","           init_checkpoint=model.ckpt-3649\n","    }\n","\n","    if [ \"$numa_nodes\" = \"1\" ];then\n","            run_inference_without_numabind\n","    else\n","            run_inference_with_numabind\n","    fi\"\"\")\n","    \n","  os.chmod(inference, 555)\n","  os.system(\"ps -Af | grep launch_benchmark.py | grep -v grep | awk '{print $2}' | xargs kill -9\")\n","  os.system(\"ps -Af | grep run_squad.py | grep -v grep | awk '{print $2}' | xargs kill -9\")\n","  p = subprocess.Popen([inference], stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","  directory_to_second_numa_info = Path(\"/sys/devices/system/node/node1\")\n","\n","  \n","  if  directory_to_second_numa_info.exists():\n","    # 2 NUMA nodes\n","    for line in iter(p.stdout.readline, ''):\n","      if b'Reading package lists...' in line or b'INFO:tensorflow:tokens' in line or b'INFO:tensorflow:  name = bert' in line:\n","        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n","      if b\"INFO:tensorflow:examples/sec\" in line:\n","        print(\"\\t\\t\\t\\t  Inference started, current real-time throughput (examples/sec) : \" + str(float(str(line).strip(\"\\\\n'\").split(' ')[1])*2), end='\\r')\n","      if b\"throughput((num_processed_examples-threshod_examples)/Elapsedtime)\" in line:\n","        print(\"\\t\\t\\t\\t  Inference finished, overall inference throughput (examples/sec) : \" + str(float(str(line).strip(\"\\\\n'\").split(':')[1])*2), end='\\r')\n","      if line == b'' and p.poll() != None:\n","        break\n","  else:\n","    # 1 NUMA node\n","    for line in iter(p.stdout.readline, ''):\n","      if b'Reading package lists...' in line or b'INFO:tensorflow:tokens' in line or b'INFO:tensorflow:  name = bert' in line:\n","        print(\"\\t\\t\\t\\t  Preparing data ......\", end='\\r')\n","      if b\"INFO:tensorflow:examples/sec\" in line:\n","        print(\"\\t\\t\\t\\t  Inference started, current real-time throughput (examples/sec) : \" + str(line).strip(\"\\\\n'\").split(' ')[1], end='\\r')\n","      if b\"throughput((num_processed_examples-threshod_examples)/Elapsedtime)\" in line:\n","        print(\"\\t\\t\\t\\t  Inference finished, overall inference throughput (examples/sec) : \" + str(line).strip(\"\\\\n'\").split(':')[1], end='\\r')\n","      if line == b'' and p.poll() != None:\n","        break\n","       \n","  p.stdout.close()\n","  \n","run_inference()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0d9d730c-68e3-4795-9402-27f829077e4c","showTitle":false,"title":""}},"source":["## Check whether is Intel-optimized TensorFlow\n","\n","This is a simple auxiliary script tool to check whether the installed TensorFlow is Intel-optimized TensorFlow. \"Ture\" represents Intel-optimized TensorFlow."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1a2eaedb-cf21-4c29-928b-ee4716c8ad20","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["# Print version, and check whether is Intel-optimized\n","import tensorflow\n","print(\"tensorflow version: \" + tensorflow.__version__)\n","\n","from packaging import version\n","if (version.parse(\"2.5.0\") <= version.parse(tensorflow.__version__)):\n","  from tensorflow.python.util import _pywrap_util_port\n","  print( _pywrap_util_port.IsMklEnabled())\n","else:\n","  from tensorflow.python import _pywrap_util_port\n","  print(_pywrap_util_port.IsMklEnabled())"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"benchmark_tensorflow_bertlarge","notebookOrigID":2536724445105255,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
