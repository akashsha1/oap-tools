{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6405d48d",
   "metadata": {},
   "source": [
    "# Run BERT-Large inference workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f631208",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download datasets, checkpoints and pre-trained model\n",
    "rm -rf ~/TF/bert-large\n",
    "mkdir -p  ~/TF/bert-large/SQuAD-1.1\n",
    "cd ~/TF/bert-large/SQuAD-1.1\n",
    "wget https://github.com/oap-project/oap-project.github.io/raw/master/resources/ai/bert/dev-v1.1.json\n",
    "wget https://github.com/oap-project/oap-project.github.io/raw/master/resources/ai/bert/evaluate-v1.1.py\n",
    "wget https://github.com/oap-project/oap-project.github.io/raw/master/resources/ai/bert/train-v1.1.json\n",
    "\n",
    "cd ~/TF/bert-large\n",
    "wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/bert_large_checkpoints.zip\n",
    "unzip bert_large_checkpoints.zip\n",
    "\n",
    "cd ~/TF/bert-large\n",
    "wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "unzip wwm_uncased_L-24_H-1024_A-16.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# BERT-Large Inference\n",
    "# Install necessary packages\n",
    "sudo apt-get install -y numactl\n",
    "# Create ckpt directory\n",
    "rm -rf  ~/TF/bert-large/inference/*\n",
    "mkdir -p ~/TF/bert-large/inference/BERT-Large-output\n",
    "# Download IntelAI benchmark\n",
    "cd ~/TF/bert-large/inference\n",
    "wget https://github.com/IntelAI/models/archive/refs/tags/v1.8.1.zip\n",
    "unzip v1.8.1.zip\n",
    "wget https://github.com/oap-project/oap-tools/raw/master/integrations/ml/databricks/benchmark/IntelAI_models_bertlarge_inference_realtime_throughput.patch\n",
    "cd ./models-1.8.1/\n",
    "git apply ../IntelAI_models_bertlarge_inference_realtime_throughput.patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "#Bert-Large Inference\n",
    "export BERT_LARGE_OUTPUT=~/TF/bert-large/inference/BERT-Large-output\n",
    "export SQUAD_DIR=~/TF/bert-large/SQuAD-1.1/\n",
    "export BERT_LARGE_DIR=~/TF/bert-large/\n",
    "export PYTHONPATH=~/TF/bert-large/inference/models-1.8.1/benchmarks/\n",
    "cd ~/TF/bert-large/inference/models-1.8.1/benchmarks/\n",
    "\n",
    "function run_inference_without_numabind() {\n",
    "    /anaconda/envs/azureml_py38_tensorflow/bin/python launch_benchmark.py \\\n",
    "    --model-name=bert_large \\\n",
    "    --precision=fp32 \\\n",
    "    --mode=inference \\\n",
    "    --framework=tensorflow \\\n",
    "    --batch-size=32 \\\n",
    "    --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n",
    "    --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n",
    "    --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n",
    "    --verbose \\\n",
    "    --infer_option=SQuAD \\\n",
    "       DEBIAN_FRONTEND=noninteractive \\\n",
    "       predict_file=$SQUAD_DIR/dev-v1.1.json \\\n",
    "       experimental-gelu=False \\\n",
    "       init_checkpoint=model.ckpt-3649\n",
    "}\n",
    "    \n",
    "\n",
    "function run_inference_with_numabind() {\n",
    "    nohup /anaconda/envs/azureml_py38_tensorflow/bin/python launch_benchmark.py \\\n",
    "    --model-name=bert_large \\\n",
    "    --precision=fp32 \\\n",
    "    --mode=inference \\\n",
    "    --framework=tensorflow \\\n",
    "    --batch-size=32 \\\n",
    "    --socket-id 0  \\\n",
    "    --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n",
    "    --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n",
    "    --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n",
    "    --verbose \\\n",
    "    --infer_option=SQuAD \\\n",
    "       DEBIAN_FRONTEND=noninteractive \\\n",
    "       predict_file=$SQUAD_DIR/dev-v1.1.json \\\n",
    "       experimental-gelu=False \\\n",
    "       init_checkpoint=model.ckpt-3649 >> socket0-inference-log &\n",
    "\n",
    "    nohup /anaconda/envs/azureml_py38_tensorflow/bin/python launch_benchmark.py \\\n",
    "    --model-name=bert_large \\\n",
    "    --precision=fp32 \\\n",
    "    --mode=inference \\\n",
    "    --framework=tensorflow \\\n",
    "    --batch-size=32 \\\n",
    "    --socket-id 1 \\\n",
    "    --data-location $BERT_LARGE_DIR/wwm_uncased_L-24_H-1024_A-16 \\\n",
    "    --checkpoint $BERT_LARGE_DIR/bert_large_checkpoints \\\n",
    "    --output-dir $BERT_LARGE_OUTPUT/bert-squad-output \\\n",
    "    --verbose \\\n",
    "    -- infer_option=SQuAD \\\n",
    "       DEBIAN_FRONTEND=noninteractive \\\n",
    "       predict_file=$SQUAD_DIR/dev-v1.1.json \\\n",
    "       experimental-gelu=False \\\n",
    "       init_checkpoint=model.ckpt-3649 >> socket1-inference-log &\n",
    "}\n",
    "    \n",
    "numa_nodes=$(lscpu | awk '/^NUMA node\\(s\\)/{ print $3 }')\n",
    "\n",
    "if [ \"$numa_nodes\" = \"1\" ];then\n",
    "        run_inference_without_numabind\n",
    "else\n",
    "        run_inference_with_numabind\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ee8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Get the inference result\n",
    "numa_nodes=$(lscpu | awk '/^NUMA node\\(s\\)/{ print $3 }')\n",
    "if [ \"$numa_nodes\" = \"1\" ];then\n",
    "        cd ~/TF/bert-large/inference/BERT-Large-output/bert-squad-output/\n",
    "        cat benchmark*.log | grep \"throughput((num_processed_examples-threshod_examples)/Elapsedtime)\"\n",
    "else\n",
    "        cd  ~/TF/bert-large/inference/models-1.8.1/benchmarks/\n",
    "        cat socket*-log | grep \"throughput((num_processed_examples-threshod_examples)/Elapsedtime)\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccabd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print TensorFlow version, and check whether is intel-optimized\n",
    "\n",
    "import tensorflow\n",
    "print(\"tensorflow version: \" + tensorflow.__version__)\n",
    "\n",
    "from packaging import version\n",
    "if (version.parse(\"2.5.0\") <= version.parse(tensorflow.__version__)):\n",
    "  from tensorflow.python.util import _pywrap_util_port\n",
    "  print( _pywrap_util_port.IsMklEnabled())\n",
    "else:\n",
    "  from tensorflow.python import _pywrap_util_port\n",
    "  print(_pywrap_util_port.IsMklEnabled())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Tensorflow",
   "language": "python",
   "name": "azureml_py38_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
