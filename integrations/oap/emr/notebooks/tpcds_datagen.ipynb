{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c532a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.extraClassPath\":\"/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:///opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\", \n",
    "        \"spark.executor.extraClassPath\":\"/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:///opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val scale = \"1\"                   // data scale 1GB\n",
    "val format = \"parquet\"            // support parquer or orc\n",
    "val partitionTables = true        // create partition table\n",
    "val storage = \"s3\"                // support hdfs or s3\n",
    "var bucket_name = \"aws-emr-resources-348941870272-us-east-2\"   // when storage is \"s3\", this value will be use.\n",
    "val useDoubleForDecimal = false   // use double format instead of decimal format\n",
    "\n",
    "if (storage == \"hdfs\"){\n",
    "    bucket_name = \"/user/livy\"    // scala notebook only has the write permission of \"hdfs:///user/livy\" directory     \n",
    "}\n",
    "\n",
    "val tools_path = \"/opt/benchmark-tools/tpcds-kit/tools\"\n",
    "val data_path = s\"${storage}://${bucket_name}/datagen/tpcds_${format}/${scale}\"\n",
    "val database_name = s\"tpcds_${format}_scale_${scale}_db\"\n",
    "val codec = \"snappy\"\n",
    "val clusterByPartitionColumns = partitionTables\n",
    "\n",
    "val p = scale.toInt / 2048.0\n",
    "val catalog_returns_p = (263 * p + 1).toInt\n",
    "val catalog_sales_p = (2285 * p * 0.5 * 0.5 + 1).toInt\n",
    "val store_returns_p = (429 * p + 1).toInt\n",
    "val store_sales_p = (3164 * p * 0.5 * 0.5 + 1).toInt\n",
    "val web_returns_p = (198 * p + 1).toInt\n",
    "val web_sales_p = (1207 * p * 0.5 * 0.5 + 1).toInt\n",
    "\n",
    "import com.databricks.spark.sql.perf.tpcds.TPCDSTables\n",
    "spark.sqlContext.setConf(s\"spark.sql.$format.compression.codec\", codec)\n",
    "\n",
    "val tables = new TPCDSTables(spark.sqlContext, tools_path, scale, useDoubleForDecimal)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"call_center\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"catalog_page\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"customer\", 6)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"customer_address\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"customer_demographics\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"date_dim\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"household_demographics\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"income_band\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"inventory\", 6)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"item\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"promotion\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"reason\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"ship_mode\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"store\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"time_dim\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"warehouse\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_page\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_site\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"catalog_sales\", catalog_sales_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"catalog_returns\", catalog_returns_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"store_sales\", store_sales_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"store_returns\", store_returns_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_sales\", web_sales_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_returns\", web_returns_p)\n",
    "tables.createExternalTables(data_path, format, database_name, overwrite = true, discoverPartitions = partitionTables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
