{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af338b8-1eae-4295-8f7e-f3dc0092d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"spark.driver.extraClassPath=file:///opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\" >> /etc/spark/conf/spark-defaults.conf\n",
    "!echo \"spark.executor.extraClassPath=file:///opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\" >> /etc/spark/conf/spark-defaults.conf\n",
    "!echo \"spark.sql.warehouse.dir=hdfs:///user/livy\">> /etc/spark/conf/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c87720-f449-4a04-a69f-d28050191da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir -p /user/livy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9fd82-a00b-4100-af74-6e5edf227a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val scale = \"1\"                   // data scale 1GB\n",
    "val format = \"parquet\"            // support parquet or orc\n",
    "val partitionTables = true        // create partition table\n",
    "val storage = \"hdfs\"              // choose HDFS\n",
    "val bucket_name = \"/user/livy\"    // scala notebook only has the write permission of \"hdfs:///user/livy\" directory    \n",
    "val useDoubleForDecimal = false   // use double format instead of decimal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753945d6-b2fe-46e7-94bc-113d48f02be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val tools_path = \"/opt/benchmark-tools/tpcds-kit/tools\"\n",
    "val data_path = s\"${storage}://${bucket_name}/datagen/tpcds_${format}/${scale}\"\n",
    "val database_name = s\"tpcds_${format}_scale_${scale}_db\"\n",
    "val codec = \"snappy\"\n",
    "val clusterByPartitionColumns = partitionTables\n",
    "\n",
    "val p = scale.toInt / 2048.0\n",
    "val catalog_returns_p = (263 * p + 1).toInt\n",
    "val catalog_sales_p = (2285 * p * 0.5 * 0.5 + 1).toInt\n",
    "val store_returns_p = (429 * p + 1).toInt\n",
    "val store_sales_p = (3164 * p * 0.5 * 0.5 + 1).toInt\n",
    "val web_returns_p = (198 * p + 1).toInt\n",
    "val web_sales_p = (1207 * p * 0.5 * 0.5 + 1).toInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fcf5a-d372-4b80-b237-20f7d6d5baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.databricks.spark.sql.perf.tpcds.TPCDSTables\n",
    "spark.sqlContext.setConf(s\"spark.sql.$format.compression.codec\", codec)\n",
    "\n",
    "val tables = new TPCDSTables(spark.sqlContext, tools_path, scale, useDoubleForDecimal)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"call_center\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"catalog_page\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"customer\", 6)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"customer_address\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"customer_demographics\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"date_dim\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"household_demographics\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"income_band\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"inventory\", 6)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"item\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"promotion\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"reason\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"ship_mode\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"store\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"time_dim\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"warehouse\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_page\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_site\", 1)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"catalog_sales\", catalog_sales_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"catalog_returns\", catalog_returns_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"store_sales\", store_sales_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"store_returns\", store_returns_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_sales\", web_sales_p)\n",
    "tables.genData(data_path, format, true, partitionTables, clusterByPartitionColumns, false, \"web_returns\", web_returns_p)\n",
    "tables.createExternalTables(data_path, format, database_name, overwrite = true, discoverPartitions = partitionTables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
